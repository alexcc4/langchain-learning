import sys
from dotenv import load_dotenv
from typing import Literal
from pydantic import BaseModel, Field

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_chroma import Chroma
from langchain.tools.retriever import create_retriever_tool
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')

# é…ç½®
CHROMA_PATH = "./chroma_data"
COLLECTION_NAME = "xiyouji_collection"
EMBEDDING_MODEL = "qwen3-embedding:latest"  
CHAT_MODEL = "qwen3:latest"  

# åˆå§‹åŒ– LLM
response_model = ChatOllama(
    model=CHAT_MODEL,
    temperature=0.2,  # é™ä½æ¸©åº¦ä½¿å›ç­”æ›´å‡†ç¡®
)

grader_model = ChatOllama(
    model=CHAT_MODEL,
    temperature=0,  # è¯„åˆ†æ—¶ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
)

# è·å–å‘é‡å­˜å‚¨
embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
vector_store = Chroma(
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings,
    persist_directory=CHROMA_PATH,
)

# åˆ›å»ºæ£€ç´¢å™¨å’Œå·¥å…·
retriever = vector_store.as_retriever(search_kwargs={"k": 3})
retriever_tool = create_retriever_tool(
    retriever,
    "retrieve_xiyouji",
    "ä»è¥¿æ¸¸è®°åŸæ–‡ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾å…³äºäººç‰©ã€æƒ…èŠ‚ã€åœ°ç‚¹ç­‰å†…å®¹ã€‚",
)

# ============= èŠ‚ç‚¹å‡½æ•° =============

def generate_query_or_respond(state: MessagesState):
    """
    æ ¹æ®å½“å‰çŠ¶æ€è°ƒç”¨ LLM ç”Ÿæˆå“åº”ã€‚
    å†³å®šæ˜¯ä½¿ç”¨æ£€ç´¢å·¥å…·ï¼Œè¿˜æ˜¯ç›´æ¥å›ç­”ç”¨æˆ·ã€‚
    """
    print("ğŸ“ èŠ‚ç‚¹: ç”ŸæˆæŸ¥è¯¢æˆ–å“åº”")
    response = (
        response_model
        .bind_tools([retriever_tool])
        .invoke(state["messages"])
    )
    return {"messages": [response]}


# æ–‡æ¡£è¯„åˆ†çš„ç»“æ„åŒ–è¾“å‡º
class GradeDocuments(BaseModel):
    """ä½¿ç”¨äºŒå…ƒè¯„åˆ†æ£€æŸ¥æ–‡æ¡£ç›¸å…³æ€§ã€‚"""
    binary_score: str = Field(
        description="ç›¸å…³æ€§è¯„åˆ†: 'yes' è¡¨ç¤ºç›¸å…³, 'no' è¡¨ç¤ºä¸ç›¸å…³"
    )


GRADE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè¯„ä¼°æ£€ç´¢æ–‡æ¡£ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³æ€§çš„è¯„åˆ†å‘˜ã€‚

æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜:
{question}

å¦‚æœæ–‡æ¡£åŒ…å«ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å…³é”®è¯æˆ–è¯­ä¹‰å†…å®¹ï¼Œåˆ™è¯„ä¸ºç›¸å…³ã€‚
ç»™å‡ºäºŒå…ƒè¯„åˆ† 'yes' æˆ– 'no' æ¥è¡¨ç¤ºæ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚
"""


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """åˆ¤æ–­æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ã€‚"""
    print("ğŸ“ èŠ‚ç‚¹: æ–‡æ¡£è¯„åˆ†")
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    prompt = GRADE_PROMPT.format(question=question, context=context)
    
    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè¿›è¡Œè¯„åˆ†
    response = (
        grader_model
        .with_structured_output(GradeDocuments)
        .invoke([{"role": "user", "content": prompt}])
    )
    
    score = response.binary_score
    print(f"   ç›¸å…³æ€§è¯„åˆ†: {score}")
    
    if score == "yes":
        return "generate_answer"
    else:
        return "rewrite_question"


REWRITE_PROMPT = """åˆ†æè¾“å…¥é—®é¢˜ï¼Œç†è§£å…¶èƒŒåçš„è¯­ä¹‰æ„å›¾å’Œå«ä¹‰ã€‚

åŸå§‹é—®é¢˜:
------- 
{question}
-------

è¯·é‡æ–°è¡¨è¿°ä¸€ä¸ªæ”¹è¿›çš„é—®é¢˜ï¼Œä½¿å…¶æ›´å®¹æ˜“æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯:
"""


def rewrite_question(state: MessagesState):
    """é‡å†™åŸå§‹ç”¨æˆ·é—®é¢˜ä»¥æé«˜æ£€ç´¢æ•ˆæœã€‚"""
    print("ğŸ“ èŠ‚ç‚¹: é—®é¢˜é‡å†™")
    messages = state["messages"]
    question = messages[0].content
    
    prompt = REWRITE_PROMPT.format(question=question)
    response = response_model.invoke([{"role": "user", "content": prompt}])
    
    print(f"   é‡å†™å: {response.content}")
    return {"messages": [{"role": "user", "content": response.content}]}


GENERATE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªè¥¿æ¸¸è®°çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚
æ ¹æ®æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ã€‚
æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ï¼Œä¿æŒå›ç­”ç®€æ´ã€‚

é—®é¢˜: {question}

æ£€ç´¢åˆ°çš„å†…å®¹: 
{context}
"""


def generate_answer(state: MessagesState):
    """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚"""
    print("ğŸ“ èŠ‚ç‚¹: ç”Ÿæˆç­”æ¡ˆ")
    question = state["messages"][0].content
    context = state["messages"][-1].content
    
    prompt = GENERATE_PROMPT.format(question=question, context=context)
    response = response_model.invoke([{"role": "user", "content": prompt}])
    
    return {"messages": [response]}


# ============= æ„å»ºå›¾ =============

workflow = StateGraph(MessagesState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("generate_query_or_respond", generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node("rewrite_question", rewrite_question)
workflow.add_node("generate_answer", generate_answer)

# æ·»åŠ è¾¹
workflow.add_edge(START, "generate_query_or_respond")

# æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦æ£€ç´¢
workflow.add_conditional_edges(
    "generate_query_or_respond",
    tools_condition,  # è¯„ä¼° LLM å†³ç­–ï¼ˆè°ƒç”¨å·¥å…·è¿˜æ˜¯ç›´æ¥å“åº”ï¼‰
    {
        "tools": "retrieve",  # å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œå‰å¾€æ£€ç´¢èŠ‚ç‚¹
        END: END,  # å¦åˆ™ç»“æŸ
    },
)

# æ¡ä»¶è¾¹ï¼šè¯„ä¼°æ£€ç´¢ç»“æœ
workflow.add_conditional_edges(
    "retrieve",
    grade_documents,  # è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§
)

workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

# ç¼–è¯‘å›¾
agent = workflow.compile()


def chat(query: str):
    """ä¸ Agentic RAG ç³»ç»Ÿå¯¹è¯"""
    print(f"\n{'='*60}")
    print(f"â“ é—®é¢˜: {query}")
    print(f"{'='*60}\n")
    
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": query}]},
    ):
        for node, update in chunk.items():
            if node != "__end__":
                print(f"\n--- æ¥è‡ªèŠ‚ç‚¹ '{node}' çš„æ›´æ–° ---")
                if update.get("messages"):
                    update["messages"][-1].pretty_print()
    
    print(f"\n{'='*60}\n")


def main():
    print("="*60)
    print("ğŸš€ è¥¿æ¸¸è®° Agentic RAG é—®ç­”ç³»ç»Ÿ")
    print("   (å¸¦æ–‡æ¡£è¯„åˆ†ä¸é—®é¢˜é‡å†™)")
    print("="*60)
    
    # æ£€æŸ¥å‘é‡åº“
    try:
        count = vector_store._collection.count()
        print(f"âœ“ å‘é‡æ•°æ®åº“å·²åŠ è½½: {count} ä¸ªæ–‡æ¡£å—\n")
    except Exception as e:
        print(f"âœ— å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œ main.py ç´¢å¼• PDF æ–‡æ¡£")
        return
    
    # æµ‹è¯•é—®é¢˜
    test_queries = [
        "å­™æ‚Ÿç©ºæ˜¯æ€ä¹ˆå‡ºç”Ÿçš„ï¼Ÿ",
    ]
    
    for query in test_queries:
        chat(query)
        print("\n")
    
    # äº¤äº’å¼é—®ç­”
    print(f"{'='*60}")
    print("ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰ï¼š")
    print(f"{'='*60}\n")
    
    while True:
        try:
            query = input("ä½ çš„é—®é¢˜: ").strip()
            if query.lower() in ['quit', 'exit', 'q', 'é€€å‡º']:
                print("å†è§ï¼")
                break
            if not query:
                continue
            chat(query)
        except KeyboardInterrupt:
            print("\n\nå†è§ï¼")
            break


if __name__ == "__main__":
    main()
