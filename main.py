import sys
import json
from pathlib import Path
import hashlib
from typing import List

from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')

# é…ç½®
PDF_PATH = "data/è¥¿æ¸¸è®°.pdf"
CHROMA_PATH = "./chroma_data"
COLLECTION_NAME = "xiyouji_collection"
EMBEDDING_MODEL = "qwen3-embedding"
BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç†çš„é¡µæ•°
PROGRESS_FILE = "./chroma_data/progress.json"

# æ–‡æœ¬åˆ†å‰²å™¨ - ä¸­æ–‡æ–‡æ¡£ä½¿ç”¨è¾ƒå°çš„ chunk_size æ›´åˆé€‚
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    add_start_index=True
)


def get_document_id(doc: Document) -> str:
    """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ IDï¼ˆé˜²æ­¢é‡å¤ï¼‰"""
    content = f"{doc.page_content}|{doc.metadata.get('source')}|{doc.metadata.get('page')}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def load_progress():
    """åŠ è½½å¤„ç†è¿›åº¦"""
    progress_path = Path(PROGRESS_FILE)
    if progress_path.exists():
        with open(progress_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"processed_pages": 0, "total_pages": 0, "processed_chunks": 0}


def save_progress(progress):
    """ä¿å­˜å¤„ç†è¿›åº¦"""
    progress_path = Path(PROGRESS_FILE)
    progress_path.parent.mkdir(parents=True, exist_ok=True)
    with open(progress_path, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def process_batch(
    pages: List[Document],
    vector_store: Chroma,
    start_page: int
) -> int:
    """æ‰¹é‡å¤„ç†é¡µé¢"""
    if not pages:
        return 0
    
    print(f"\nå¤„ç†é¡µé¢ {start_page} - {start_page + len(pages) - 1} ({len(pages)} é¡µ)")
    
    # åˆ†å‰²æ–‡æ¡£
    splits = text_splitter.split_documents(pages)
    print(f"  åˆ†å‰²æˆ {len(splits)} ä¸ªæ–‡æ¡£å—")
    
    if not splits:
        return 0
    
    # ç”Ÿæˆå”¯ä¸€ IDï¼ˆè‡ªåŠ¨å»é‡ï¼‰
    ids = [get_document_id(split) for split in splits]
    
    # æ‰¹é‡æ·»åŠ 
    print(f"  æ­£åœ¨ç”Ÿæˆ embeddings å¹¶å­˜å‚¨...")
    try:
        vector_store.add_documents(documents=splits, ids=ids)
        print(f"  âœ“ æˆåŠŸæ·»åŠ  {len(splits)} ä¸ªæ–‡æ¡£å—")
    except Exception as e:
        print(f"  âœ— æ·»åŠ å¤±è´¥: {e}")
        raise
    
    return len(splits)


def load_and_index_pdf(pdf_path: str, batch_size: int = BATCH_SIZE):
    """
    å¢é‡åŠ è½½å’Œç´¢å¼• PDFï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    """
    # åˆ›å»ºæˆ–åŠ è½½å‘é‡å­˜å‚¨
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    vector_store = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_PATH,
    )
    
    # åŠ è½½è¿›åº¦
    progress = load_progress()
    start_page = progress.get("processed_pages", 0)
    
    # åŠ è½½ PDF
    print(f"æ­£åœ¨åŠ è½½ PDF: {pdf_path}")
    loader = PyPDFLoader(pdf_path)
    all_pages = list(loader.lazy_load())  # æ‡’åŠ è½½
    total_pages = len(all_pages)
    print(f"âœ“ PDF å…± {total_pages} é¡µ")
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    if start_page >= total_pages:
        print(f"âœ“ æ‰€æœ‰é¡µé¢å·²å¤„ç†å®Œæˆï¼ˆå…± {progress['processed_chunks']} ä¸ªæ–‡æ¡£å—ï¼‰")
        return vector_store
    
    # æ˜¾ç¤ºç»§ç»­å¤„ç†ä¿¡æ¯
    if start_page > 0:
        print(f"ğŸ“ ä»ç¬¬ {start_page} é¡µç»§ç»­ï¼ˆå·²å¤„ç† {progress['processed_chunks']} ä¸ªæ–‡æ¡£å—ï¼‰")
    
    # æ‰¹é‡å¤„ç†
    progress["total_pages"] = total_pages
    current_page = start_page
    total_chunks = progress.get("processed_chunks", 0)
    
    try:
        while current_page < total_pages:
            end_page = min(current_page + batch_size, total_pages)
            batch = all_pages[current_page:end_page]
            
            # å¤„ç†è¿™ä¸€æ‰¹
            chunks_added = process_batch(batch, vector_store, current_page)
            total_chunks += chunks_added
            
            # æ›´æ–°è¿›åº¦
            current_page = end_page
            progress["processed_pages"] = current_page
            progress["processed_chunks"] = total_chunks
            save_progress(progress)
            
            # æ˜¾ç¤ºè¿›åº¦
            percent = (current_page / total_pages) * 100
            print(f"  ğŸ“Š è¿›åº¦: {current_page}/{total_pages} ({percent:.1f}%) - ç´¯è®¡ {total_chunks} ä¸ªæ–‡æ¡£å—")
        
        print(f"\nâœ“ æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆï¼å…± {total_chunks} ä¸ªæ–‡æ¡£å—")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼è¿›åº¦å·²ä¿å­˜")
        print(f"   å·²å¤„ç†åˆ°ç¬¬ {current_page} é¡µï¼Œä¸‹æ¬¡è¿è¡Œå°†ç»§ç»­")
        return vector_store
    
    return vector_store


def get_vector_store():
    """è·å–ç°æœ‰çš„å‘é‡å­˜å‚¨"""
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    return Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_PATH,
    )


def search_documents(vector_store, query, k=3):
    """æœç´¢ç›¸å…³æ–‡æ¡£"""
    print(f"\næŸ¥è¯¢: {query}")
    results = vector_store.similarity_search_with_score(query, k=k)
    
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:\n")
    for i, (doc, score) in enumerate(results, 1):
        print(f"--- ç»“æœ {i} (ç›¸ä¼¼åº¦: {score:.4f}) ---")
        print(f"å†…å®¹: {doc.page_content[:150]}...")
        print(f"é¡µç : {doc.metadata.get('page', 'N/A')}")
        print()
    
    return results


def main():
    print("="*60)
    print("PDF è¯­ä¹‰æœç´¢å¼•æ“")
    print("="*60)
    
    # åŠ è½½å’Œç´¢å¼•ï¼ˆè‡ªåŠ¨æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    vector_store = load_and_index_pdf(PDF_PATH)
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    try:
        count = vector_store._collection.count()
        print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡: {count} ä¸ªæ–‡æ¡£å—")
    except:
        pass
    
    # æµ‹è¯•æœç´¢
    print("\n" + "="*60)
    print("æµ‹è¯•æœç´¢")
    print("="*60)
    
    search_documents(vector_store, "å­™æ‚Ÿç©ºçš„æ¥å†", k=2)
    search_documents(vector_store, "å”åƒ§å–ç»", k=2)
    
    print("\n" + "="*60)
    print("ğŸ’¡ æç¤º:")
    print("  - ç¨‹åºæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ŒCtrl+C ä¸­æ–­åé‡æ–°è¿è¡Œä¼šç»§ç»­å¤„ç†")
    print("  - è‡ªåŠ¨å»é‡ï¼Œé‡å¤è¿è¡Œä¸ä¼šäº§ç”Ÿå†—ä½™æ•°æ®")
    print("  - å¦‚éœ€é‡æ–°ç´¢å¼•ï¼Œåˆ é™¤ chroma_data ç›®å½•")
    print("="*60)


if __name__ == "__main__":
    main()
